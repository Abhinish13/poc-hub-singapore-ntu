# Troubleshooting Issues

Issues could arise during the deployment of JupyterHub, or during its operation. A number of issues are described below along with how to deal with them. How to retrieve logs for applications and notification events from OpenShift are also covered.

## Viewing Application Logs

Processes for the JupyterHub application and PostgreSQL database, along with processes for each separate instance of a users Jupyter notebook, all run in distinct containers (pods) under OpenShift. The pods isolate the applications from each other, as well as constraining what the applications or users of those applications can do.

Any log output from the respective applications sent to stdout/stderr is captured by OpenShift and can be retrieved as necessary. To retrieve the logs for an application, you need to know the name of the pod for the current instance of the application.

To get a list of the pods in a project for a course, you can run:

```
$ oc get pods -n coursename
```

The output will be similar to:

```
NAME                         READY     STATUS      RESTARTS   AGE
jupyterhub-1-5wzf5           1/1       Running     0          3m
jupyterhub-database-1-jpst6  1/1       Running     0          5m
jupyterhub-nb-username       1/1       Running     0          1m
```

The pods are identified by the prefix in the name.

* ``jupyterhub`` - The main JupyterHub application.
* ``jupyterhub-database`` - The PostgreSQL database.
* ``jupyter-nb`` - A Jupyter notebook instance for a user.

To list the pod for just the JupyterHub instance, run:

```
$ oc get pods --selector app=jupyterhub,deploymentconfig=jupyterhub -n coursename
```

To list the pod for just the PostgreSQL database, run:

```
$ oc get pods --selector app=jupyterhub,deploymentconfig=jupyterhub-database -n coursename
```

To list the pods for the Jupyter notebook instances, run:

```
$ oc get pods --selector app=jupyterhub,component=singleuser-server -n coursename
```

Once you have identified the pod for the application instance you want to see the logs for, you can run ``oc logs`` with the name of the pod as argument.

```
$ oc logs jupyterhub-nb-username -n coursename
```

This will output the current saved log output. Note that older log output will eventually be truncated.

To monitor log output as it is generated by the application, you can add the ``--follow`` option.

```
$ oc logs jupyterhub-nb-username --follow -n coursename
```

## Viewing OpenShift Events

Application logs are the output from the running application. OpenShift also logs details about deployment of applications, and the process of building images, as events.

To view the current saved events for a project, run:

```
$ oc get events -n coursename
```

To watch the event stream as it is being generated, you can add the ``--watch`` option.

```
$ oc get events --watch -n coursename
```

## Image Builds Failing

When JupyterHub is deployed, a number of builds are set up. These include both Source-to-Image (S2I) builds and docker type builds. The builds can fail due to networking issues due to the fact that the builds will pull down software packages from repositories such as the Python package index (PyPi) and Node package manager repository.

The list of build configurations created are:

* ``jupyterhub-hub-s2i`` - Creates a base image for JupyterHub. The result is an S2I builder image that can be run against a repository to incorporate custom configuration for a JupyterHub deployment with the base image.
* ``jupyterhub-hub-img`` - Creates a custom JupyterHub image, by running the ``jupyterhub-hub-s2i`` image as an S2I builder against the [jupyterhub](../jupyterhub) directory of this repository.
* ``jupyterhub-nb-s2i`` - Creates a base image running Jupyter notebooks. The result is an S2I builder image that can be run against a repository to incorporate a set of Jupyter notebooks with the base image, and installed required packages.
* ``jupyterhub-nb-bld`` - Creates a custom ``jupyterhub-nb-s2i`` image using a docker type build. This is to permit additional system packages for libraries required by Python packages to be installed.
* ``jupyterhub-nb-img`` - Creates a custom Jupyter notebook image, by running the ``jupyterhub-nb-bld`` image against the specifed remote Git repository containing the Jupyter notebooks for the course.

If a build fails, it is not necessary to destroy and recreate the deployment. Instead the failing build can be restarted.

On the command line, use ``oc get builds`` to determine which build failed.

```
$ oc get builds -n coursename
```

To view the build logs and determine why the build may have failed, use ``oc logs`` with the name of the build has argument.

```
$ oc logs build/jupyterhub-hub-img-1 -n coursename
```

To restart a build, run ``oc start-build`` on the corresponding build configuration. For example:

```
$ oc start-build jupyterhub-hub-img --follow -n coursename
```

The ``--follow`` option is optional, and will result in the build being monitored, with log file output appearing in the terminal.

You can see a list of the build configurations by running ``oc get bc``.

```
$ oc get bc -n coursename
```

The failed build can also be restarted from the web console by going to the list of builds, selecting on the one which is failing and clicking on _Start Build_.


## Corrupt Database

JupyterHub uses a PostgreSQL database to store the current list of whitelisted users and admin users, along with the state of any current active sessions.

If this database were to become corrupted, the easiest way to recover is to start over with a fresh database. To be able to do this, you need to have ensured you have kept up to date the config maps for the admin users and user whitelist. That is, if you had manually added users, or designated users as admins, through the admin panel of JupyterHub, that you had also updated the corresponding config maps. So long as you do this, the database can be re-created from the lists provided by the config maps.

A script is provided to rollover to a new database instance in one action, however, step by step instructions are also detailed below so the process can be understood in case the process needs to be adapted based on specific circumstances.

If using the script, run:

```
sudo scripts/rollover-database.sh coursename 2
```

The arguments to the script should be the course name and the version number to be used to identified the persistent storage for the next instance of the database deployment.

The individual steps if not using the all in one script, are as follows.

First up, if you don't have a copy of the config maps for the admin users and user whitelist saved separate to the OpenShift cluster, make one. This can be done by running the commands:

```
$ scripts/extract-admin-users-backup.sh coursename > admin_users.txt
```

and

```
$ scripts/extract-user-whitelist-backup.sh coursename > user_whitelist.txt
```

This is a backup copy just in case required.

Next ensure that both PostgreSQL and JupyterHub are not running. This can be done by running the commands:

```
$ oc scale --replicas=0 dc/jupyterhub -n coursename
```

and:

```
$ oc scale --replicas=0 dc/jupyterhub-database -n coursename
```

The ``-n`` argument ensures you are performing the step against the correct project for the course.

Now ensure there are no Jupyter notebook instances running.

```
$ oc delete pods --selector component=singleuser-server -n coursename
```

From the host where you are able to mount the NFS volumes, run:

```
$ sudo scripts/create-database-directory.sh coursename 2
```

This should result in a new directory being created with format ``database-$COURSE_NAME-pv2``. That is, it will create a directory parallel to the existing directory for the database, but with ``2`` at the end.

Next create the corresponding persistent volume resource definition.

```
$ scripts/create-database-volume.sh coursename 2
```

This should result in a persistent volume resource definition with name ``$COURSE_NAME-database-pv2`` being created.

It is now necessary to unmount the existing persistent volume claim from the deployment configuration for the PostgreSQL database.

```
$ oc set volume dc/jupyterhub-database --remove --name data -n coursename
```

Now create a new persistent volume claim associated with the new persistent volume.

```
$ oc process -f templates/database-claim.json \
  --param COURSE_NAME=coursename \
  --param VERSION_NUMBER=2 | oc create -f - -n coursename
```

This should result in the creation of a peristent volume claim in the project for the course with format ``jupyterhub-database-pvc2``.

The persistent volume can now be mounted against PostgreSQL using:

```
$ oc set volume dc/jupyterhub-database --add \
  --claim-name jupyterhub-database-pvc2 --name data \
  --mount-path /var/lib/pgsql/data -n coursename
```

Set PostgreSQL running again by running:

```
$ oc scale --replicas=1 dc/jupyterhub-database -n coursename
```

Confirm that PostgreSQL starts up okay.

Then start up JupyterHub again.

```
$ oc scale --replicas=1 dc/jupyterhub -n coursename
```

Note that if you had to use this procedure, you will have multiple database directories and persistent volume definitions for the one course. When later deleting the course, you will need to remember to delete all versions of the directories and persistent volume definitions.

## Project Deletion

Each course is deployed to a separate project. The project should only be deleted when the course has completed and the JupyterHub instance for that course is no longer required.

If the project is accidentally deleted, this will result in the removal of the JupyterHub and PostgreSQL database instances. In deleting the project, although the persistent volume claims will have been deleted, the underlying persistent volumes will still be intact, as the reclaim policy for the persistent volumes has been set to ``Retain``, meaning that the contents will not be discarded even if the persistent volume claim has been deleted.

To recreate the project and restore the JupyterHub and PostgreSQL database instances, the steps outlined below need to be followed.

As a safeguard to ensure you don't loose existing data in the notebooks and database directories, a copy should be made of these directories. The new instance of JupyterHub will use the copy and not the original.

Mount the NFS share for database directories. From the root directory of the mounted file system, where the sub directories for each course are located, identify the database directory for the course which needs to be recovered. This will have a name of the form:

```
database-coursename-pv
```

You should make a copy of this directory by running:

```
sudo rsync -av ./database-coursename-pv ./database-coursename-pv2
```

The ``2`` represents a version number. You will use this number is subsequent commands.

Unmount the NFS share for the database directories.

Mount the NFS share for the notebooks directories.

Similar to above, you will make a copy of the existing notebooks directory.

The name of the directory for the notebooks directory will have the form:

```
notebooks-coursename-pv
```

Make a copy of this using the same version number as used above.

```
sudo rsync -av ./notebooks-coursename-pv ./notebooks-coursename-pv2
```

Keep the NFS shared mounted at this point as it is necessary to fix up ownership on files below.

Next you need to create the persistent volume definitions for the copies of the directories.

```
scripts/create-database-volume.sh coursename 2
scripts/create-notebooks-volume.sh coursename 2
```

The arguments are the course name and the version number used above.

You can now create the project for the course.

```
scripts/create-project.sh coursename
```

and create the JupyterHub and PostgreSQL instances.

With the project created, work out what user ID is assigned to the project. To find out this, run:

```
oc describe project coursename
```

This will output various information. Look for the ``openshift.io/sa.scc.uid-range`` annotation. For example:

```
openshift.io/sa.scc.uid-range=1014260000/10000
```

You now need to change the owner of the database and notebook directories in the NFS share that you created when you copied the originals, to be the user UID given in the ``uid-range``.

In this example the user ID is '1014260000'. Make sure you use what ``oc describe project`` shows for the course project.

Find the directories which are owned by the prior user ID for the old project that was deleted, and then run ``chown -R`` on them, setting the user ID to that from ``uid-range``. For example, for the database directory run:

```
chown -R 1014260000 userdata
```

and for notebooks directory run:

```
chown -R 1014260000 users backups
```

You can now unmount the NFS share.


```
scripts/instantiate-template.sh coursename
```

You will be prompted for details of the Git repository, LDAP credentials and any permanent admin users. You will also be prompted for the version number. You must supply the same version number you used above.

Note that if you had to use this procedure, you will have multiple database and notebooks directories, and corresponding persistent volume definitions, for the one course. When later deleting the course, you will need to remember to delete all versions of the directories and persistent volume definitions.

## Load Testing

If needing to load test the JupyterHub deployment and OpenShift environment to see if you can create many concurrent Jupyter notebook instances at the same time, you will first need to disable the LDAP authentication mechanism for users.

From the web console, edit the config map ``jupyterhub-cfg`` and under the ``jupyterhub_config.py`` entry add:

```
c.JupyterHub.authenticator_class = 'tmpauthenticator.TmpAuthenticator'
```

Also override the default timeout for idle Jupyter notebook instances so that they will be cleaned up after a shorter period than the default of 60 minutes (3600 seconds).


```
$ oc set env dc/jupyterhub JUPYTERHUB_IDLE_TIMEOUT=300
```

Running ``oc set env`` will cause a redeployment of JupyterHub and the updated config map will now also be used.

You can now use curl to create Jupyter notebook instances. A script for testing can be found at [scripts/spawn-jupyter-notebooks.sh](../scripts/spawn-jupyter-notebooks.sh).

Run the script as:

```
$ scripts/spawn-jupyter-notebooks.sh https://... 5 5
```

The arguments are the URL for the JupyterHub instance, the number of sessions to create and the delay between each session being created.
